## Overview
This repository is the implementation for the paper "VISIONARY: Vision-aware enhancement with reminding scenes generated by captions via multimodal transformer for embodied referring expression".

Authors: Zhengwu Yuan, Peixian Tang, Fan Zhang, Zheqi Zhang

![image](overview.png)
## Requirements
1. Please follow the baseline work [LAD](https://github.com/zehao-wang/LAD) to complete the environment preparation and download of related data.

2. Download the additional generated content for VISIONARY from [here](https://drive.google.com/drive/folders/15aPIDUTwWqhRqX5Zp-doJg0j6-pwjavs?usp=drive_link), and put the data in 'datasets' directory.

## Training
After configuring the training strategy, run the following script to train:
```
cd training_src
sh scripts/final_frt_gd_finetuning_stable.sh
```

#### Todo
- [x] Upload the code of VISIONARY.
- [x] Upload the additional generated content.
- [ ] Sort out and upload the code of data preprocessing.
- [ ] Upload the final checkpoints.

## Acknowledgement
The code is mainly based on [LAD](https://github.com/zehao-wang/LAD), [DUET](https://github.com/cshizhe/VLN-DUET). Thanks for their awesome works!


